{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoyVu8P5Z_Eq"
      },
      "source": [
        "#Setup\n",
        "Install rlcard and import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bESzOkDZZ4JX",
        "outputId": "d28e4371-1432-46ce-ba64-354120f3ab97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rlcard\n",
            "  Downloading rlcard-1.0.9.tar.gz (265 kB)\n",
            "\u001b[K     |████████████████████████████████| 265 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.8/dist-packages (from rlcard) (1.21.6)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from rlcard) (2.1.1)\n",
            "Building wheels for collected packages: rlcard\n",
            "  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rlcard: filename=rlcard-1.0.9-py3-none-any.whl size=322178 sha256=a0132d6705b3c9b2d48be538ca8d53abb3f40f08c0c8c40e7995de1ded7ad855\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/6c/14/931032d53068211d4e0ee697f24844b90652600fd5c91544c2\n",
            "Successfully built rlcard\n",
            "Installing collected packages: rlcard\n",
            "Successfully installed rlcard-1.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rlcard\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUH64oNhaD1c"
      },
      "outputs": [],
      "source": [
        "import rlcard\n",
        "\n",
        "import torch\n",
        "import torchinfo\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRqcK7Y1aNoh"
      },
      "source": [
        "#Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3liO425aNNb"
      },
      "outputs": [],
      "source": [
        "class CardInput(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CardInput, self).__init__()\n",
        "    # 60 for my hand, 60 for visible card\n",
        "    self.layer1 = torch.nn.Linear(120, 164)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class HiddenNetwork(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(HiddenNetwork, self).__init__()\n",
        "\n",
        "    self.layer1 = torch.nn.Linear(164, 224)\n",
        "    self.layer2 = torch.nn.Linear(224, 86)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.layer2(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class ActorBlock(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ActorBlock, self).__init__()\n",
        "\n",
        "    self.layer1 = torch.nn.Linear(86, 61)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = torch.nn.functional.softmax(x, dim=0)\n",
        "\n",
        "    return x\n",
        "  \n",
        "class CriticBlock(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CriticBlock, self).__init__()\n",
        "\n",
        "    self.layer1 = torch.nn.Linear(86, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEefi4m7iAfY"
      },
      "outputs": [],
      "source": [
        "class UnoChamp(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(UnoChamp, self).__init__()\n",
        "\n",
        "    self.cardInput = CardInput()\n",
        "\n",
        "    self.hiddneNetwork = HiddenNetwork()\n",
        "\n",
        "    self.actorBlock = ActorBlock()\n",
        "    self.criticBlock = CriticBlock()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.cardInput(x)\n",
        "\n",
        "    x = self.hiddneNetwork(x)\n",
        "\n",
        "    actor = self.actorBlock(x)\n",
        "    critic = self.criticBlock(x)\n",
        "\n",
        "    return (critic, actor)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OEni4SM8cbv",
        "outputId": "66486397-4057-4d94-f4c7-fcfe14f2fe21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "UnoChamp                                 [1, 1]                    --\n",
              "├─CardInput: 1-1                         [1, 164]                  --\n",
              "│    └─Linear: 2-1                       [1, 164]                  19,844\n",
              "├─HiddenNetwork: 1-2                     [1, 86]                   --\n",
              "│    └─Linear: 2-2                       [1, 224]                  36,960\n",
              "│    └─Linear: 2-3                       [1, 86]                   19,350\n",
              "├─ActorBlock: 1-3                        [1, 61]                   --\n",
              "│    └─Linear: 2-4                       [1, 61]                   5,307\n",
              "├─CriticBlock: 1-4                       [1, 1]                    --\n",
              "│    └─Linear: 2-5                       [1, 1]                    87\n",
              "==========================================================================================\n",
              "Total params: 81,548\n",
              "Trainable params: 81,548\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.08\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.33\n",
              "Estimated Total Size (MB): 0.33\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "torchinfo.summary(UnoChamp(), input_size=(1, 120))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO70IPe6ws9D"
      },
      "source": [
        "#Custom Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kP9RLlowsJK"
      },
      "outputs": [],
      "source": [
        "class MultiLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiLoss, self).__init__()\n",
        "\n",
        "    def forward(self, log_probs, expected_advantage, advantage):\n",
        "        log_probs = np.array(log_probs)\n",
        "        actor_loss = log_probs.sum() * abs(expected_advantage - advantage).item()\n",
        "        critic_loss = torch.nn.functional.huber_loss(expected_advantage, advantage)\n",
        "\n",
        "        total_loss = actor_loss + (critic_loss * 0.5)\n",
        "\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M078KIK_I50"
      },
      "source": [
        "#Q Learning Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Wa3BZl_cVC"
      },
      "outputs": [],
      "source": [
        "class QLearnAgent():\n",
        "  def __init__(self):\n",
        "    self.use_raw = False # required for RL Card Env \n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model = UnoChamp().to(self.device)\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=3e-3)\n",
        "    self.num_actions = 61\n",
        "\n",
        "  # Action taken when learning=False\n",
        "  def step(self, state): \n",
        "    action, _ = self.eval_step(state)\n",
        "    return action\n",
        "\n",
        "  # Action taken when learning=True\n",
        "  def eval_step(self, state):\n",
        "    cards = self.clean_state(state)\n",
        "    value, raw_probs = self.model(cards)\n",
        "\n",
        "    value = value.cpu()\n",
        "    raw_probs = raw_probs.detach().cpu().numpy()\n",
        "\n",
        "    probs = []\n",
        "    for i in range(self.num_actions):\n",
        "      if i in state['legal_actions'].keys():\n",
        "        probs.append(raw_probs[i])\n",
        "      else: \n",
        "        probs.append(0)\n",
        "\n",
        "    choice = np.argmax(probs)\n",
        "\n",
        "    return choice, probs\n",
        "\n",
        "  def update(self, state, next_state, log_probs, reward):\n",
        "        state = self.clean_state(state)\n",
        "        next_state = self.clean_state(next_state)\n",
        "\n",
        "        value, *_ = self.model(state[None, ...])\n",
        "        next_value, *_ = self.model(next_state[None, ...])\n",
        "        td_target = reward + next_value\n",
        "\n",
        "        loss = MultiLoss()(log_probs, td_target, value)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.detach().cpu().item()\n",
        "    \n",
        "  def clean_state(self, state):\n",
        "    cards = state['obs']\n",
        "    cards = np.array([cards[1], cards[3]]).flatten()\n",
        "    cards = torch.tensor(cards).float().to(self.device)\n",
        "\n",
        "    return cards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RvZXyy9_M2t"
      },
      "source": [
        "#Random Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgEsxt4u_Oqt"
      },
      "outputs": [],
      "source": [
        "class RandomAgent():\n",
        "  def __init__(self, num_actions):\n",
        "    self.num_actions = num_actions\n",
        "    self.use_raw = False \n",
        "  \n",
        "  def step(self, state):\n",
        "    return random.choice(list(state['legal_actions']))\n",
        "  \n",
        "  def eval_step(self, state):\n",
        "    probs = []\n",
        "    for i in range(self.num_actions):\n",
        "      if i in state['legal_actions'].keys():\n",
        "        probs.append(1/len(state['legal_actions']))\n",
        "      else: \n",
        "        probs.append(0)\n",
        "\n",
        "    # List all the probabilities for each actions, this will help with training for non random agents\n",
        "\n",
        "    return self.step(state), probs\n",
        "\n",
        "  def update(self, state, next_state, log_probs, reward):\n",
        "    pass\n",
        "\n",
        "  def clean_state(self, state):\n",
        "    cards = state['obs']\n",
        "    cards = np.array([cards[1], cards[3]]).flatten()\n",
        "    cards = torch.tensor(cards).float().to(self.device)\n",
        "\n",
        "    return cards\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igQhZt0zjU6p"
      },
      "source": [
        "# Defining The Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLl8kWXEzfCe"
      },
      "outputs": [],
      "source": [
        "env = rlcard.make(\"uno\", config={'seed': 1})\n",
        "\n",
        "learningAgent = QLearnAgent()\n",
        "randomAgent = RandomAgent(env.num_actions)\n",
        "\n",
        "env.set_agents([learningAgent, randomAgent])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7i0G2muAXQ3"
      },
      "source": [
        "# Useful Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVdbv8qbAWyL"
      },
      "outputs": [],
      "source": [
        "def playGame(seed=1, learn=False):\n",
        "  env = rlcard.make(\"uno\", config={'seed': seed})\n",
        "  env.set_agents([learningAgent, randomAgent])\n",
        "\n",
        "  state, player_id = env.reset()\n",
        "  action, probs = env.agents[player_id].eval_step(state)\n",
        "\n",
        "  last_states = [state, state]\n",
        "  last_probs = [probs, probs]\n",
        "\n",
        "  # Loop to play the game\n",
        "  turns = [0, 0]\n",
        "  while not env.is_over():\n",
        "    # Agent plays\n",
        "    action, probs = env.agents[player_id].eval_step(state)\n",
        "\n",
        "    # Environment steps\n",
        "    next_state, next_player_id = env.step(action, env.agents[player_id].use_raw)\n",
        "    turns[player_id] += 1\n",
        "\n",
        "    reward = 0\n",
        "    if env.is_over():\n",
        "      reward = 1\n",
        "    if learn:\n",
        "      env.agents[player_id].update(state, next_state, probs, reward)\n",
        "\n",
        "    last_states[player_id] = state\n",
        "    last_probs[player_id] = probs\n",
        "    # Set the state and player\n",
        "    state = next_state\n",
        "    player_id = next_player_id\n",
        "\n",
        "  if env.get_payoffs()[0] == 1:\n",
        "    player_id = 1\n",
        "  else:\n",
        "    player_id = 0\n",
        "  env.get_state(player_id)\n",
        "  reward = -1\n",
        "  if learn:\n",
        "    env.agents[player_id].update(last_states[player_id], state, last_probs[player_id], reward)\n",
        "  \n",
        "  payoffs = env.get_payoffs()\n",
        "\n",
        "\n",
        "  return payoffs, turns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzt7NcPt_AEw"
      },
      "outputs": [],
      "source": [
        "def distributionGraph(percentages, title=\"\"):\n",
        "  mean = np.average(percentages)\n",
        "  top = 30\n",
        "\n",
        "  plt.vlines(mean, ymin=0, ymax=top, color=\"g\")\n",
        "  plt.hist(percentages, bins=10, range=(0.0, 1.0) )\n",
        "  plt.xlabel(f\"Win Percentage (Average: {mean})\")\n",
        "  plt.ylabel(\"Number of seeds in bin\")\n",
        "  plt.ylim(top=top)\n",
        "  plt.title(title)\n",
        "  \n",
        "  plt.show()\n",
        "  return mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXIvrn4_YjOX"
      },
      "outputs": [],
      "source": [
        "def winPercentageGraphs(seeds=100, games=10, display=False, title=\"\", seedStart=0):\n",
        "  winningPerc=[]\n",
        "  colors=[]\n",
        "  scoreboard = [0,0]\n",
        "  seedClassify = [0, 0, 0]\n",
        "  for i in tqdm(range(seedStart, (games * seeds) + seedStart)):\n",
        "    seed = i // games\n",
        "    payoff, _ = playGame(seed=seed, learn=False)\n",
        "    if payoff[0] == 1:\n",
        "      scoreboard[0]+= 1\n",
        "    else: \n",
        "      scoreboard[1] += 1\n",
        "    if (i+1) % games == 0:\n",
        "      if display:\n",
        "        print(f\"seed: {i//games:4d}\\t Scoreboard: {scoreboard}\" )\n",
        "      perc = scoreboard[0] / games\n",
        "      if perc >= 0.70:\n",
        "        seedClassify[2] += 1\n",
        "        colors.append('green')\n",
        "      elif perc <= 0.40: \n",
        "        seedClassify[0] += 1\n",
        "        colors.append('red')\n",
        "      else: \n",
        "        seedClassify[1] += 1\n",
        "        colors.append('blue')\n",
        "      winningPerc.append(perc)\n",
        "      scoreboard = [0, 0]\n",
        "  print(f\"Losing  Seeds:{seedClassify[2]:3d}\")\n",
        "  print(f\"Neutral Seeds:{seedClassify[1]:3d}\")\n",
        "  print(f\"Winning Seeds:{seedClassify[0]:3d}\")\n",
        "\n",
        "  mean = distributionGraph(winningPerc, title)\n",
        "\n",
        "  return mean\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTx7SMjvGt3g"
      },
      "source": [
        "# Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2okbTApGtsT"
      },
      "outputs": [],
      "source": [
        "baselineGames = 50\n",
        "seeds = 100\n",
        "trainingGames=5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nANwzoxKadfs"
      },
      "source": [
        "# Before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twxbmNTPZiW8"
      },
      "outputs": [],
      "source": [
        "winPercentageGraphs(seeds=seeds, games=baselineGames, title=\"Before Training\")\n",
        "winPercentageGraphs(seeds=seeds, games=baselineGames, title=\"Before Training, Excluded Seeds\", seedStart=seeds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiUIsi9pblTV"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D1hA6triZNl"
      },
      "outputs": [],
      "source": [
        "def trainModel(seeds=100, games=10, display=False):\n",
        "  winPerc = []\n",
        "  scoreboard = [0,0]\n",
        "  seedClassify = [0, 0, 0]\n",
        "  for i in tqdm(range(games * seeds)):\n",
        "    seed = i // games\n",
        "    payoff, turns = playGame(seed=seed, learn=True)\n",
        "    if payoff[0] == 1:\n",
        "      scoreboard[0]+= 1\n",
        "    else: \n",
        "      scoreboard[1] += 1\n",
        "    if (i+1) % games == 0:\n",
        "      if display:\n",
        "        print(f\"seed: {i//games:4d}\\t Scoreboard: {scoreboard}\" )\n",
        "      if scoreboard[0] / games >= 0.70:\n",
        "        seedClassify[2] += 1\n",
        "      elif scoreboard[0] / games <= 0.40: \n",
        "        seedClassify[0] += 1\n",
        "      else: \n",
        "        seedClassify[1] += 1\n",
        "\n",
        "      winPerc.append(scoreboard[0] / games)\n",
        "      scoreboard = [0, 0]\n",
        "\n",
        "\n",
        "  return seedClassify, winPerc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW2L92FtfibQ"
      },
      "outputs": [],
      "source": [
        "means=[]\n",
        "\n",
        "for i in range(15):\n",
        "  _ ,wins = trainModel(seeds=seeds, games=trainingGames)\n",
        "  mean = distributionGraph(wins,  title=f\"Epoch {i}\")\n",
        "  means.append(mean)\n",
        "\n",
        "plt.plot(means)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5hD9vFwD5m7"
      },
      "source": [
        "# After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCUfPf8lD3iR"
      },
      "outputs": [],
      "source": [
        "winPercentageGraphs(seeds=seeds, games=baselineGames, title=\"After Training\")\n",
        "winPercentageGraphs(seeds=seeds, games=baselineGames, title=\"After Training, Excluded Seeds\", seedStart=seeds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "yoyVu8P5Z_Eq",
        "XRqcK7Y1aNoh",
        "wO70IPe6ws9D",
        "3M078KIK_I50",
        "0RvZXyy9_M2t"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}